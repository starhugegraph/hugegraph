# gremlin entrance to create graph
# none auth config: com.baidu.hugegraph.HugeFactory
# auth config: com.baidu.hugegraph.auth.HugeFactoryAuthProxy
gremlin.graph=com.baidu.hugegraph.HugeFactory

# The data store type, allow: rocksdb, cassandra, hbase, mysql, palo, postgresql and etc.
backend=rocksdb

# The serializer for backend store, like: text/binary/cassandra
serializer=binary

# The database name like Cassandra Keyspace
store=hugegraph

# The system table name, which store system data
#store.system=s

# The schema table name, which store meta data
#store.schema=m

# The graph table name, which store vertex, edge and property
#store.graph=g

raft.mode=false
raft.safe_read=false
raft.use_snapshot=false
raft.endpoint=127.0.0.1:8281
raft.group_peers=127.0.0.1:8281,127.0.0.1:8282,127.0.0.1:8283
raft.path=./raft-log
raft.use_replicator_pipeline=true
raft.election_timeout=10000
raft.snapshot_interval=3600
raft.backend_threads=48
raft.read_index_threads=8
raft.queue_size=16384
raft.queue_publish_timeout=60
raft.apply_batch=1
raft.rpc_threads=80
raft.rpc_connect_timeout=5000
raft.rpc_timeout=60000

# The max rate(items/s) to add/update/delete vertices/edges
#rate_limit.write=0

# The max rate(times/s) to execute query of vertices/edges
#rate_limit.read=0

# Timeout in seconds for waiting for the task to complete, such as when truncating or clearing the backend
# task.wait_timeout=10

# The job input size limit in bytes
#task.input_size_limit=

# The job result size limit in bytes
#task.result_size_limit=

# The batch size used to delete expired data
#task.ttl_delete_batch=1

# Whether to delete schema or expired data synchronously
#task.sync_deletion=false

# The interval in seconds for detecting connections, if the idle time of a connection exceeds this value
# detect it and reconnect if needed before using, value 0 means detecting every time
#store.connection_detect_interval=600

# The default vertex label
#vertex.default_label=vertex

# Whether to check the vertices exist for those using customized id strategy
#vertex.check_customized_id_exist=false

# Whether remove left index at overwrite
#vertex.remove_left_index_at_overwrite=false

# Whether to check the adjacent vertices of edges exist
#vertex.check_adjacent_vertex_exist=false

# Whether to lazy load adjacent vertices of edges
#vertex.lazy_load_adjacent_vertex=true

# Whether to enable the mode to commit part of edges of vertex, enabled if commit size > 0, 0 meas disabled
#vertex.part_edge_commit_size=5000

# Whether to encode number value of primary key in vertex id
#vertex.encode_primary_key_number=true

# The max size(items) of vertices(uncommitted) in transaction
#vertex.tx_capacity=10000

# The max size(items) of edges(uncommitted) in transaction
#edge.tx_capacity=10000

# Whether to ignore invalid data of vertex or edge
#query.ignore_invalid_data=true

# Whether to optimize aggregate query(like count) by index
#query.optimize_aggregate_by_index=false

# The size of each batch when querying by batch
#query.batch_size=1000

# The size of each page when querying by paging
#query.page_size=500

# The maximum number of intermediate results to ]intersect indexes when querying by multiple single index properties
#query.index_intersect_threshold=1000

# Whether to enable ramtable for query of adjacent edges
#query.ramtable_enable=false

# The maximum number of vertices in ramtable, generally the largest vertex id is used as capacity
#query.ramtable_vertices_capacity=10000000

# The maximum number of edges in ramtable, include OUT and IN edges
#query.ramtable_edges_capacity=20000000

# The regex specified the illegal format for schema name
#schema.illegal_name_regex=\s+|~.*

# The max cache size(items) of schema cache
#schema.cache_capacity=10000

# The type of vertex cache, allowed values are [l1, l2]
#vertex.cache_type=l1

# The max cache size(items) of vertex cache
#vertex.cache_capacity=10000000
# The expiration time in seconds of vertex cache
#vertex.cache_expire=600

# The type of edge cache, allowed values are [l1, l2]
#edge.cache_type=l1

# The max cache size(items) of edge cache
#edge.cache_capacity=1000000
# The expiration time in seconds of edge cache
#edge.cache_expire=600

# The worker id of snowflake id generator
#snowflake.worker_id=0

# The datacenter id of snowflake id generator
#snowflake.datecenter_id=0

# Whether to force the snowflake long id to be a string
#snowflake.force_string=false

# Choose a text analyzer for searching the vertex/edge properties, available type are [word, ansj, hanlp, smartcn, jieba, jcseg, mmseg4j, ikanalyzer]
# Specify the mode for the text analyzer, the available mode of analyzer are
# word: [MaximumMatching, ReverseMaximumMatching, MinimumMatching, ReverseMinimumMatching, BidirectionalMaximumMatching, BidirectionalMinimumMatching, BidirectionalMaximumMinimumMatching, FullSegmentation, MinimalWordCount, MaxNgramScore, PureEnglish]
# ansj: [BaseAnalysis, IndexAnalysis, ToAnalysis, NlpAnalysis]
# hanlp: [standard, nlp, index, nShort, shortest, speed]
# smartcn: []
# jieba: [SEARCH, INDEX]
# jcseg: [Simple, Complex]
# mmseg4j: [Simple, Complex, MaxWord]
# ikanalyzer: [smart, max_word]
search.text_analyzer=jieba
search.text_analyzer_mode=INDEX

# Thread number to concurrently execute oltp algorithm
#oltp.concurrent_threads=10

# The min depth to enable concurrent oltp algorithm
#oltp.concurrent_depth=10

# The implementation type of collections used in oltp algorithm, allow: JCF, EC, FU
#oltp.collection_type=EC


# ROCKSDB BACKEND CONFIG
# The path for storing data & storing WAL of RocksDB, default: rocksdb-data
#rocksdb.data_path=/path/to/disk
#rocksdb.wal_path=/path/to/disk

# The optimized disks for storing data of RocksDB. The format of each element: `STORE/TABLE: /path/disk`.
# Allowed keys are:
# [g/vertex, g/edge_out, g/edge_in, g/vertex_label_index, g/edge_label_index, g/range_int_index, g/range_float_index,
#  g/range_long_index, g/range_double_index, g/secondary_index, g/search_index, g/shard_index, g/unique_index, g/olap]
#rocksdb.data_disks=

# The path for ingesting SST file into RocksDB
#rocksdb.sst_path=

# The info log level of RocksDB: DEBUG, INFO, WARN, ERROR, FATAL, HEADER
#rocksdb.log_level=INFO

# Set the number of levels for this database, default: 7
#rocksdb.num_levels=3

# Set compaction style for RocksDB: LEVEL/UNIVERSAL/FIFO.
#rocksdb.compaction_style=LEVEL

# Optimize for heavy workloads and big datasets
#rocksdb.optimize_mode=true

# Switch to the mode to bulk load data into RocksDB
#rocksdb.bulkload_mode=false

# The compression algorithms for different levels of RocksDB, allowed values are none/snappy/z/bzip2/lz4/lz4hc/xpress/zstd
# default: [none, none, snappy, snappy, snappy, snappy, snappy]
#rocksdb.compression_per_level=[none, none, none]

# The compression algorithm for the bottommost level of RocksDB, allowed values are none/snappy/z/bzip2/lz4/lz4hc/xpress/zstd
#rocksdb.bottommost_compression=none

# The compression algorithm for compressing blocks of RocksDB, allowed values are none/snappy/z/bzip2/lz4/lz4hc/xpress/zstd
#rocksdb.compression=snappy

# Maximum number of concurrent background jobs, including flushes and compactions, default: 8
#rocksdb.max_background_jobs=32

# The value represents the maximum number of threads per compaction job, default: 4
#rocksdb.max_subcompactions=8

# The rate limit in bytes/s of user write requests when need to slow down if the compaction gets behind, default: 16L * Bytes.MB
#rocksdb.delayed_write_rate=64000000

# The maximum number of open files that can be cached by RocksDB, -1 means no limit, default: -1
#rocksdb.max_open_files=40960

# The max size of manifest file in bytes
#rocksdb.max_manifest_file_size=104857600

# Whether to skip statistics update when opening the database, setting this flag true allows us to not update statistics
#rocksdb.skip_stats_update_on_db_open=false

# The max number of threads used to open files
#rocksdb.max_file_opening_threads=16

# Total size of WAL files in bytes. Once WALs exceed this size, we will start forcing the flush of column families related, 0 means no limit
#rocksdb.max_total_wal_size=0

# Total size of write buffers in bytes across all column families, 0 means no limit
#rocksdb.db_write_buffer_size=0

# enable unordered write
#rocksdb.unordered_write=false

# enable pipeline write, default: false
#rocksdb.enable_pipeline_write=true

# disable wal for rocksdb, default: false
#rocksdb.disable_wal=false

# set reuse log file num for saving io-ops, default: 0
#rocksdb.recycle_log_file_num=1

# wait for flush, default: false
#rocksdb.wait_for_flush=true

# The periodicity in seconds when obsolete files get deleted, 0 means always do full purge
#rocksdb.delete_obsolete_files_period=21600

# Amount of data in bytes to build up in memory, default: 128L * Bytes.MB
#rocksdb.write_buffer_size=256000000

# The maximum number of write buffers that are built up in memory, default: 6
#rocksdb.max_write_buffer_number=128

# The minimum number of write buffers that will be merged together, default: 2
#rocksdb.min_write_buffer_number_to_merge=32

# The total maximum number of write buffers to maintain in memory
#rocksdb.max_write_buffer_number_to_maintain=0

# Whether to enable level_compaction_dynamic_level_bytes, if it's enabled we give max_bytes_for_level_multiplier a
# priority against max_bytes_for_level_base, the bytes of base level is dynamic for a more predictable LSM tree,
# it is useful to limit worse case space amplification. Turning this feature on/off for an existing DB can cause
# unexpected LSM tree structure so it's not recommended
#rocksdb.level_compaction_dynamic_level_bytes=false

# The upper-bound of the total size of level-1 files in bytes, default: 512L * Bytes.MB
#rocksdb.max_bytes_for_level_base=81920000000

# The ratio between the total size of level (L+1) files and the total size of level L files for all L
#rocksdb.max_bytes_for_level_multiplier=10

# The target file size for compaction in bytes, default: 64L * Bytes.MB
#rocksdb.target_file_size_base=2048000000

# The size ratio between a level L file and a level (L+1) file, default: 1
#rocksdb.target_file_size_multiplier=10

# Number of files to trigger level-0 compaction, default: 2
#rocksdb.level0_file_num_compaction_trigger=20

# Soft limit on number of level-0 files for slowing down writes, default: 20
#rocksdb.level0_slowdown_writes_trigger=10000

# Hard limit on number of level-0 files for stopping writes, default: 36
#rocksdb.level0_stop_writes_trigger=10000

# The soft limit to impose on pending compaction in bytes, default: 68719476736
#rocksdb.soft_pending_compaction_bytes_limit=100000000000000

# The hard limit to impose on pending compaction in bytes, default: 274877906944
#rocksdb.hard_pending_compaction_bytes_limit=100000000000000

# Allow the OS to mmap file for writing
#rocksdb.allow_mmap_writes=false

# Allow the OS to mmap file for reading sst tables
#rocksdb.allow_mmap_reads=false

# Enable the OS to use direct I/O for reading sst tables
#rocksdb.use_direct_reads=false

# Enable the OS to use direct read/writes in flush and compaction
#rocksdb.use_direct_io_for_flush_and_compaction=false

# The amount of block cache in bytes that will be used by RocksDB, 0 means no block cache.
#rocksdb.block_cache_capacity=8388608

# Indicating if we'd put index/filter blocks to the block cache
#rocksdb.pin_l0_filter_and_index_blocks_in_cache=false

# Indicating if we'd put index/filter blocks to the block cache
#rocksdb.cache_index_and_filter_blocks=false

# The bits per key in bloom filter, a good value is 10, which yields a filter with ~ 1% false positive rate, -1 means no bloom filter
#rocksdb.bloom_filter_bits_per_key=-1

# Use block based filter rather than full filter
#rocksdb.bloom_filter_block_based_mode=false

# True if place whole keys in the bloom filter, else place the prefix of keys
#rocksdb.bloom_filter_whole_key_filtering=true

# This flag allows us to not store filters for the last level
#rocksdb.optimize_filters_for_hits=false

# cassandra backend config
cassandra.host=localhost
cassandra.port=9042
cassandra.username=
cassandra.password=
#cassandra.connect_timeout=5
#cassandra.read_timeout=20
#cassandra.keyspace.strategy=SimpleStrategy
#cassandra.keyspace.replication=3

# hbase backend config
#hbase.hosts=localhost
#hbase.port=2181
#hbase.znode_parent=/hbase
#hbase.threads_max=64

# mysql backend config
#jdbc.driver=com.mysql.jdbc.Driver
#jdbc.url=jdbc:mysql://127.0.0.1:3306
#jdbc.username=root
#jdbc.password=
#jdbc.reconnect_max_times=3
#jdbc.reconnect_interval=3
#jdbc.sslmode=false

# postgresql & cockroachdb backend config
#jdbc.driver=org.postgresql.Driver
#jdbc.url=jdbc:postgresql://localhost:5432/
#jdbc.username=postgres
#jdbc.password=
#jdbc.postgresql.connect_database=template1

# palo backend config
#palo.host=127.0.0.1
#palo.poll_interval=10
#palo.temp_dir=./palo-data
#palo.file_limit_size=32
